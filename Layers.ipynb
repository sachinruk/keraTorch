{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers\n",
    "> All the basic layers used keratorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from fastai.vision import *\n",
    "from fastai import layers\n",
    "\n",
    "from keraTorch.activations import *\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class __inputDimError__(Exception):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, input_shape=None):\n",
    "#         breakpoint()\n",
    "        self.input_shape = input_shape\n",
    "        \n",
    "    def __set_io_shape__(self, input_shape):\n",
    "        if input_shape is None and self.input_shape is None:\n",
    "            __inputDimError__(\"Need to specify input shape in first layer\")\n",
    "        elif input_shape:\n",
    "            self.input_shape = input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense\n",
    "Linear layer that takes in `input_dim` and converts it to `units` number of dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Dense:\n",
    "    def __init__(self, units, input_dim=None, activation=None, \n",
    "                 use_bias=True, kernel_regularizer=None, bias_regularizer=None, \n",
    "                 activity_regularizer=None):\n",
    "        \"\"\"\n",
    "        Linear layer that takes in `input_dim` and converts it to `units` number of dimensions.\n",
    "        parameters:\n",
    "        - units: output dimension.\n",
    "        - input_dim: input dimension.\n",
    "        - activation (optional): non-linear activation.\n",
    "        - use_bias (optional): To include bias layer or not (default: True)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.activation = get_activation(activation) if activation else None\n",
    "        if input_dim:\n",
    "            self.layer = nn.Linear(input_dim, units, bias=use_bias)\n",
    "        else:\n",
    "            self.layer = partial(nn.Linear, out_features=units, bias=use_bias)\n",
    "        # TODO: implement regularizers\n",
    "        \n",
    "    def get_layer(self, input_dim=None):\n",
    "        if input_dim is None and self.input_dim is None:\n",
    "            __inputDimError__(\"Need to specify number of input dimensions in first layer\")\n",
    "        elif input_dim:\n",
    "            self.input_dim = input_dim\n",
    "            self.layer = self.layer(in_features=input_dim)\n",
    "        # else self.layer is already is assigned\n",
    "        \n",
    "        self.output_dim = self.layer.out_features\n",
    "        layers = [layer for layer in [self.layer, self.activation] if layer]\n",
    "        \n",
    "        return {'output_dim': self.output_dim, 'layers': layers}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_dim': 3,\n",
       " 'layers': [Linear(in_features=5, out_features=3, bias=True), Mish()]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hide\n",
    "Dense(3, 5, activation='mish').get_layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_dim': 3,\n",
       " 'layers': [Linear(in_features=5, out_features=3, bias=True), Mish()]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dense(3, activation='mish').get_layer(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Conv2D:\n",
    "    def __init__(self, filters:int, kernel_size:int=3, strides:int=1, padding:int=None, \n",
    "                 activation:str=None, use_bias:bool=True, input_shape:tuple=None):\n",
    "        \"\"\"\n",
    "        Apply convolution on image using kernel filters.\n",
    "        parameters:\n",
    "        - filters: number of kernel filters\n",
    "        - kernel_size: the width of the (square) kernel\n",
    "        - strides: number of pixels to skip when sliding kernel (default 1)\n",
    "        - padding: number of pixels to pad incoming image ` defaults to `ks//2`\n",
    "        - activation: non-linearity\n",
    "        - use_bias: bias\n",
    "        - input_shape: incoming image shape of (#Channels, width, height)\n",
    "        \"\"\"\n",
    "        self.input_shape = input_shape\n",
    "        if input_shape:\n",
    "            ni = input_shape[0]\n",
    "            self.layer = conv2d(ni, filters, kernel_size, strides, padding, use_bias)\n",
    "        else:\n",
    "            self.layer = partial(conv2d, nf=filters, ks=kernel_size, \n",
    "                                 stride=strides, padding=padding, bias=use_bias)\n",
    "        self.activation = get_activation(activation) if activation else None\n",
    "        \n",
    "    def get_layer(self, input_shape=None):\n",
    "        if input_shape is None and self.input_shape is None:\n",
    "            __inputDimError__(\"Need to specify input shape in first layer\")\n",
    "        elif input_shape:\n",
    "            self.input_shape = input_shape\n",
    "            ni = self.input_shape[0]\n",
    "            self.layer = self.layer(ni=ni)\n",
    "        # else self.input_shape is already is assigned\n",
    "\n",
    "        dummy_x = torch.zeros(self.input_shape).unsqueeze(0)\n",
    "        self.output_shape = self.layer(dummy_x).shape[1:]\n",
    "        layers = [layer for layer in [self.layer, self.activation] if layer]\n",
    "        \n",
    "        return {'output_dim': self.output_shape, 'layers': layers}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output_dim': torch.Size([5, 10, 10]), 'layers': [Conv2d(1, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True)]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 10, 10])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_layer = Conv2D(5, activation='Relu', input_shape=(1, 10, 10)).get_layer()\n",
    "print(out_layer)\n",
    "conv_layer = out_layer['layers'][0]\n",
    "conv_layer(torch.zeros((1, 1, 10, 10))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output_dim': torch.Size([5, 10, 10]), 'layers': [Conv2d(1, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True)]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 10, 10])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_layer = Conv2D(5, activation='Relu').get_layer((1, 10, 10))\n",
    "print(out_layer)\n",
    "conv_layer = out_layer['layers'][0]\n",
    "conv_layer(torch.zeros((1, 1, 10, 10))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class MaxPool2D:\n",
    "    def __init__(self, kernel_size:int=2, strides:int=None, padding:int=0, \n",
    "                 input_shape:tuple=None):\n",
    "        \"\"\"\n",
    "        Apply convolution on image using kernel filters.\n",
    "        parameters:\n",
    "        - filters: number of kernel filters\n",
    "        - kernel_size: the width of the (square) kernel\n",
    "        - strides: number of pixels to skip when sliding kernel (default 1)\n",
    "        - padding: number of pixels to pad incoming image ` defaults to `ks//2`\n",
    "        - activation: non-linearity\n",
    "        - use_bias: bias\n",
    "        - input_shape: incoming image shape of (#Channels, width, height)\n",
    "        \"\"\"\n",
    "        self.input_shape = input_shape\n",
    "        self.layer = nn.MaxPool2d(kernel_size, strides, padding)\n",
    "        \n",
    "        \n",
    "    def get_layer(self, input_shape=None):\n",
    "        if input_shape is None and self.input_shape is None:\n",
    "            __inputDimError__(\"Need to specify input shape in first layer\")\n",
    "        elif input_shape:\n",
    "            self.input_shape = input_shape\n",
    "        # else self.input_shape is already is assigned\n",
    "\n",
    "        dummy_x = torch.zeros(self.input_shape).unsqueeze(0)\n",
    "        self.output_shape = self.layer(dummy_x).shape[1:]\n",
    "        layers = [self.layer]\n",
    "        \n",
    "        return {'output_dim': self.output_shape, 'layers': layers}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output_dim': torch.Size([1, 5, 5]), 'layers': [MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 5, 5])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_layer = MaxPool2D(2, input_shape=(1, 10, 10)).get_layer()\n",
    "print(out_layer)\n",
    "maxpool_layer = out_layer['layers'][0]\n",
    "maxpool_layer(torch.zeros((1, 1, 10, 10))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output_dim': torch.Size([1, 5, 5]), 'layers': [MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 5, 5])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_layer = MaxPool2D(2).get_layer((1, 10, 10))\n",
    "print(out_layer)\n",
    "maxpool_layer = out_layer['layers'][0]\n",
    "maxpool_layer(torch.zeros((1, 1, 10, 10))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Flatten:\n",
    "    def __init__(self, input_shape=None):\n",
    "        self.layer = layers.Flatten()\n",
    "        self.input_shape = input_shape\n",
    "        \n",
    "    def get_layer(self, input_shape=None):\n",
    "        if input_shape is None and self.input_shape is None:\n",
    "            __inputDimError__(\"Need to specify input shape in first layer\")\n",
    "        elif input_shape:\n",
    "            self.input_shape = input_shape\n",
    "        # else self.input_shape is already is assigned\n",
    "\n",
    "        self.output_dim = np.prod(self.input_shape)\n",
    "        layers = [self.layer]\n",
    "        \n",
    "        return {'output_dim': self.output_dim, 'layers': layers}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_dim': 15, 'layers': [Flatten()]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten = Flatten((5, 3))\n",
    "flatten.get_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Activation:\n",
    "    def __init__(self, activation, input_shape=None):\n",
    "        self.layer = get_activation(activation)\n",
    "        self.input_shape = input_shape\n",
    "        self.output_dim = input_shape\n",
    "        \n",
    "    def get_layer(self, input_shape=None):\n",
    "        if input_shape is None and self.input_shape is None:\n",
    "            __inputDimError__(\"Need to specify input shape in first layer\")\n",
    "        elif input_shape:\n",
    "            self.input_shape = input_shape\n",
    "            self.output_dim = input_shape\n",
    "        # else self.input_shape is already is assigned\n",
    "\n",
    "        layers = [self.layer]\n",
    "        \n",
    "        return {'output_dim': self.output_dim, 'layers': layers}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_dim': None, 'layers': [Softmax(dim=-1)]}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Activation('softmax').get_layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted Activations.ipynb.\n",
      "Converted Layers.ipynb.\n",
      "Converted Model.ipynb.\n",
      "Converted cifar.ipynb.\n",
      "Converted data.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted losses.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
